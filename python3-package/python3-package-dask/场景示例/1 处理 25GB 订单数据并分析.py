from dask.distributed import Client
import dask.dataframe as dd
import logging
import time
from typing import Optional, Dict
import os

# ç«¯åˆ°ç«¯å·¥ä½œæµï¼šå¤„ç† 25GB è®¢å•æ•°æ®å¹¶åˆ†æ

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def start_dask_client(n_workers: int = 4) -> Optional[Client]:
    """
    å¯åŠ¨ Dask åˆ†å¸ƒå¼å®¢æˆ·ç«¯ã€‚

    :param n_workers: å·¥ä½œçº¿ç¨‹æ•°
    :return: Client å®ä¾‹ æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        client = Client(n_workers=n_workers)
        logger.info("âœ… å·²å¯åŠ¨åˆ†å¸ƒå¼è°ƒåº¦å™¨")
        return client
    except Exception as e:
        logger.error(f"å¯åŠ¨ Dask å®¢æˆ·ç«¯å¤±è´¥ï¼š{e}")
        return None


def load_data(input_pattern: str,
              file_format: str = 'csv',
              dtypes: Optional[Dict] = None,
              parse_dates: Optional[list] = None) -> Optional[dd.DataFrame]:
    """
    åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆCSV/Parquetï¼‰ã€‚

    :param input_pattern: æ–‡ä»¶è·¯å¾„æˆ–é€šé…ç¬¦ï¼ˆå¦‚ orders_*.csvï¼‰
    :param file_format: æ–‡ä»¶æ ¼å¼ ('csv' æˆ– 'parquet')
    :param dtypes: å­—æ®µç±»å‹æ˜ å°„
    :param parse_dates: éœ€è¦è§£æä¸ºæ—¥æœŸçš„å­—æ®µ
    :return: Dask DataFrame æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        if file_format == 'csv':
            return dd.read_csv(input_pattern, dtype=dtypes, parse_dates=parse_dates)
        elif file_format == 'parquet':
            return dd.read_parquet(input_pattern)
        else:
            raise ValueError("ä»…æ”¯æŒ csv æˆ– parquet æ ¼å¼")
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®å¤±è´¥ï¼š{e}")
        return None


def clean_data(df: dd.DataFrame, filter_column: str, threshold: float = 0) -> Optional[dd.DataFrame]:
    """
    æ•°æ®æ¸…æ´—ï¼šè¿‡æ»¤å¼‚å¸¸å€¼ã€‚

    :param df: åŸå§‹æ•°æ®
    :param filter_column: è¦è¿‡æ»¤çš„åˆ—å
    :param threshold: è¿‡æ»¤é˜ˆå€¼
    :return: æ¸…æ´—åçš„æ•°æ® æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        return df[df[filter_column] > threshold]
    except Exception as e:
        logger.error(f"æ•°æ®æ¸…æ´—å¤±è´¥ï¼š{e}")
        return None


def aggregate_daily_sales(df: dd.DataFrame,
                          group_column: str,
                          agg_column: str) -> Optional[dd.Series]:
    """
    æŒ‰æ—¥æœŸèšåˆé”€å”®é¢ã€‚

    :param df: æ¸…æ´—åçš„æ•°æ®
    :param group_column: åˆ†ç»„å­—æ®µï¼ˆå¦‚ order_dateï¼‰
    :param agg_column: èšåˆå­—æ®µï¼ˆå¦‚ priceï¼‰
    :return: æ¯æ—¥é”€å”®é¢ æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        return df.groupby(group_column)[agg_column].sum()
    except Exception as e:
        logger.error(f"èšåˆè®¡ç®—å¤±è´¥ï¼š{e}")
        return None


def save_cleaned_data(df: dd.DataFrame, output_file: str, file_format: str = 'parquet') -> bool:
    """
    ä¿å­˜æ¸…æ´—åçš„æ•°æ®ã€‚

    :param df: æ¸…æ´—åçš„ Dask DataFrame
    :param output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :param file_format: è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤ parquetï¼‰
    :return: æˆåŠŸä¸å¦
    """
    try:
        if file_format == 'parquet':
            df.to_parquet(output_file)
        elif file_format == 'csv':
            df.to_csv(output_file, single_file=True)
        else:
            raise ValueError("ä»…æ”¯æŒ parquet æˆ– csv æ ¼å¼")
        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ {output_file}")
        return True
    except Exception as e:
        logger.error(f"æ•°æ®ä¿å­˜å¤±è´¥ï¼š{e}")
        return False


def run_pipeline(
    input_dir: str = './data',          # æ–°å¢è¾“å…¥ç›®å½•å‚æ•°
    input_prefix: str = 'orders', # æ”¯æŒä¸­æ–‡å‰ç¼€
    input_suffix: str = 'csv',          # è¾“å…¥æ ¼å¼ï¼šcsv æˆ– parquet
    output_cleaned: str = 'cleaned_orders.parquet',
    output_daily_sales: str = 'daily_sales.csv',
    dtypes: Optional[Dict] = None,
    parse_dates: Optional[list] = None,
    filter_column: str = 'price',
    threshold: float = 0.0,
    group_column: str = 'order_date',
    agg_column: str = 'price'
) -> None:
    """
    æ‰§è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æ•°æ®åˆ†ææµç¨‹ã€‚
    """
    start_time = time.time()

    # å¯åŠ¨å®¢æˆ·ç«¯
    # client = start_dask_client(n_workers=4)
    # if client is None:
    #     return

    # æ„å»ºè¾“å…¥æ¨¡å¼
    input_pattern = os.path.join(input_dir, f"{input_prefix}*.{input_suffix}")

    # é˜¶æ®µ1ï¼šåŠ è½½æ•°æ®
    load_start = time.time()
    ddf = load_data(input_pattern, file_format=input_suffix, dtypes=dtypes, parse_dates=parse_dates)
    if ddf is None:
        return
    load_end = time.time()
    logger.info(f"â±ï¸ æ•°æ®åŠ è½½è€—æ—¶ï¼š{load_end - load_start:.2f} ç§’")

    # é˜¶æ®µ2ï¼šæ•°æ®æ¸…æ´—
    clean_start = time.time()
    ddf_clean = clean_data(df=ddf, filter_column=filter_column, threshold=threshold)
    if ddf_clean is None:
        return
    clean_end = time.time()
    logger.info(f"â±ï¸ æ•°æ®æ¸…æ´—è€—æ—¶ï¼š{clean_end - clean_start:.2f} ç§’")

    # é˜¶æ®µ3ï¼šä¿å­˜æ¸…æ´—åæ•°æ®
    save_start = time.time()
    save_cleaned_data(df=ddf_clean, output_file=output_cleaned, file_format='parquet')
    save_end = time.time()
    logger.info(f"â±ï¸ æ•°æ®ä¿å­˜è€—æ—¶ï¼š{save_end - save_start:.2f} ç§’")

    # é˜¶æ®µ4ï¼šèšåˆé”€å”®æ•°æ®
    agg_start = time.time()
    daily_sales = aggregate_daily_sales(df=ddf_clean, group_column=group_column, agg_column=agg_column)
    if daily_sales is None:
        return
    agg_end = time.time()
    logger.info(f"â±ï¸ èšåˆè®¡ç®—è€—æ—¶ï¼š{agg_end - agg_start:.2f} ç§’")

    # é˜¶æ®µ5ï¼šæ‰§è¡Œè®¡ç®—å¹¶å¯¼å‡ºç»“æœ
    compute_start = time.time()
    result = daily_sales.compute()
    result.to_csv(output_daily_sales)
    compute_end = time.time()
    logger.info(f"â±ï¸ ç»“æœå¯¼å‡ºè€—æ—¶ï¼š{compute_end - compute_start:.2f} ç§’")

    total_time = time.time() - start_time
    logger.info(f"âœ… æ€»è€—æ—¶ï¼š{total_time:.2f} ç§’")

    # æ¸…ç†èµ„æº
    # client.close()


if __name__ == "__main__":
    # ç¤ºä¾‹é…ç½®ï¼ˆæ ¹æ®ä½ çš„å®é™…è·¯å¾„ä¿®æ”¹ï¼‰
    dtypes = {'order_id': 'int64', 'price': 'float64'}
    parse_dates = ['order_date']

    run_pipeline(
        input_dir=r"./data",                  # ä¸­æ–‡è·¯å¾„ä¹Ÿæ”¯æŒ
        input_prefix="orders",                 # åŒ¹é… orders_*.csv
        input_suffix="parquet",                    # è¾“å…¥æ ¼å¼ï¼šcsv æˆ– parquet
        dtypes=dtypes,
        parse_dates=parse_dates,
        filter_column='price',
        group_column='order_date',
        agg_column='price',
        output_cleaned='cleaned_orders.parquet',
        output_daily_sales='daily_sales.csv'
    )
