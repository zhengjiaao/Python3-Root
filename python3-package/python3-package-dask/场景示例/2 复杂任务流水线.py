from dask import delayed
from typing import List, Optional
import logging
import time
import json
from dask.distributed import Client

# åŠ¨æ€ä»»åŠ¡ä¾èµ–ç®¡ç†ï¼šå¤æ‚ä»»åŠ¡æµæ°´çº¿ï¼ˆå¦‚ Aâ†’Bâ†’Cä¾èµ–ã€[1, 2, 3] â†’ [2, 4, 6]ä¾èµ–ï¼‰

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@delayed
def load_data(file_path: Optional[str] = None) -> List[int]:
    """
    å»¶è¿ŸåŠ è½½æ•°æ®ï¼Œæ”¯æŒä»å†…å­˜æˆ–æ–‡ä»¶åŠ è½½ã€‚

    :param file_path: æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›åˆ™ä»æ–‡ä»¶åŠ è½½ï¼‰
    :return: æ•°æ®åˆ—è¡¨
    """
    try:
        if file_path:
            with open(file_path, 'r') as f:
                data = json.load(f)
            logger.info(f"âœ… ä» {file_path} åŠ è½½æ•°æ®æˆåŠŸ")
            return data
        else:
            data = [1, 2, 3]
            logger.info("âœ… ä½¿ç”¨é»˜è®¤æ•°æ®åŠ è½½æˆåŠŸ")
            return data
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥ï¼š{e}")
        raise


@delayed
def process_data(data: List[int], multiplier: int = 2) -> List[int]:
    """
    å»¶è¿Ÿå¤„ç†æ•°æ®ï¼ˆä¾‹å¦‚å¯¹æ¯ä¸ªå…ƒç´ ä¹˜ä»¥ä¸€ä¸ªæ•°ï¼‰ã€‚

    :param data: è¾“å…¥æ•°æ®
    :param multiplier: ä¹˜æ•°å› å­
    :return: å¤„ç†åæ•°æ®
    """
    try:
        result = [i * multiplier for i in data]
        logger.info(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼š{data} â†’ {result}")
        return result
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å¤„ç†å¤±è´¥ï¼š{e}")
        raise


@delayed
def save_results(results: List[int], output_file: Optional[str] = None) -> bool:
    """
    å»¶è¿Ÿä¿å­˜ç»“æœåˆ°æ§åˆ¶å°æˆ–æ–‡ä»¶ã€‚

    :param results: ç»“æœæ•°æ®
    :param output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›åˆ™å†™å…¥æ–‡ä»¶ï¼‰
    :return: æˆåŠŸä¸å¦
    """
    try:
        if output_file:
            with open(output_file, 'w') as f:
                json.dump(results, f)
            logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ {output_file}")
        else:
            logger.info(f"ğŸ“¤ è¾“å‡ºç»“æœï¼š{results}")
        return True
    except Exception as e:
        logger.error(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥ï¼š{e}")
        return False


def run_pipeline(
    input_file: Optional[str] = None,
    output_file: Optional[str] = None,
    use_distributed: bool = False
):
    """
    æ‰§è¡Œå®Œæ•´çš„ä»»åŠ¡æµæ°´çº¿ã€‚

    :param input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
    :param output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :param use_distributed: æ˜¯å¦å¯ç”¨åˆ†å¸ƒå¼è°ƒåº¦å™¨
    """
    client = None
    if use_distributed:
        try:
            client = Client(n_workers=4)
            logger.info("âœ… å·²å¯åŠ¨åˆ†å¸ƒå¼è°ƒåº¦å™¨")
        except Exception as e:
            logger.warning(f"âš ï¸ å¯åŠ¨åˆ†å¸ƒå¼è°ƒåº¦å™¨å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ¨¡å¼è¿è¡Œï¼š{e}")

    start_time = time.time()

    # é˜¶æ®µ1ï¼šåŠ è½½æ•°æ®
    data = load_data(input_file)

    # é˜¶æ®µ2ï¼šå¤„ç†æ•°æ®
    processed = process_data(data, multiplier=2)

    # é˜¶æ®µ3ï¼šä¿å­˜ç»“æœ
    result = save_results(processed, output_file)

    # æ‰§è¡Œæµæ°´çº¿
    success = result.compute()

    total_time = time.time() - start_time
    logger.info(f"â±ï¸ æ€»è€—æ—¶ï¼š{total_time:.2f} ç§’")
    logger.info(f"âœ… æµæ°´çº¿æ‰§è¡Œ {'æˆåŠŸ' if success else 'å¤±è´¥'}")

    if client:
        client.close()


if __name__ == "__main__":
    # ç¤ºä¾‹æ–‡ä»¶è·¯å¾„ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
    input_file = "input_data.json"
    output_file = "output_result.json"

    # ç¤ºä¾‹è¾“å…¥æ•°æ®
    example_input = [1, 2, 3]
    with open(input_file, 'w') as f:
        json.dump(example_input, f)

    # è¿è¡Œæµæ°´çº¿
    run_pipeline(input_file=input_file, output_file=output_file, use_distributed=False)
