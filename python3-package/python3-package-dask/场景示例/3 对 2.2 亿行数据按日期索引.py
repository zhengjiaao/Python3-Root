from dask.distributed import Client
import dask.dataframe as dd
import logging
import time
from typing import Optional


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def start_dask_client(memory_limit: str = '32GB') -> Optional[Client]:
    """
    å¯åŠ¨ Dask åˆ†å¸ƒå¼å®¢æˆ·ç«¯ã€‚

    :param memory_limit: æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹çš„å†…å­˜é™åˆ¶
    :return: Client å®ä¾‹ æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        client = Client(memory_limit=memory_limit)
        logger.info("âœ… å·²å¯åŠ¨ Dask åˆ†å¸ƒå¼å®¢æˆ·ç«¯")
        return client
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨ Dask å®¢æˆ·ç«¯å¤±è´¥ï¼š{e}")
        return None


def load_parquet_data(file_path: str, engine: str = 'pyarrow') -> Optional[dd.DataFrame]:
    """
    åŠ è½½ Parquet æ ¼å¼çš„ DataFrameã€‚

    :param file_path: æ–‡ä»¶è·¯å¾„
    :param engine: ä½¿ç”¨çš„å¼•æ“ï¼ˆé»˜è®¤ pyarrowï¼‰
    :return: Dask DataFrame æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        df = dd.read_parquet(file_path, engine=engine)
        logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼š{file_path}")
        return df
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼š{e}")
        return None


def repartition_data(df: dd.DataFrame, partition_size: str = '100MB') -> Optional[dd.DataFrame]:
    """
    é‡æ–°åˆ†åŒºä»¥é€‚åº”å†…å­˜å‹åŠ›ã€‚

    :param df: è¾“å…¥çš„ Dask DataFrame
    :param partition_size: æ¯ä¸ªåˆ†å—å¤§å°
    :return: é‡æ–°åˆ†åŒºåçš„ DataFrame æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        df_repart = df.repartition(partition_size=partition_size)
        logger.info(f"âœ… æ•°æ®å·²é‡æ–°åˆ†åŒºï¼Œåˆ†åŒºå¤§å°ï¼š{partition_size}")
        return df_repart
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é‡æ–°åˆ†åŒºå¤±è´¥ï¼š{e}")
        return None


def set_date_index(df: dd.DataFrame, index_column: str = 'date', sort: bool = True) -> Optional[dd.DataFrame]:
    """
    è®¾ç½®æ—¥æœŸåˆ—ä½œä¸ºç´¢å¼•ï¼Œå¹¶å¯ç”¨æ’åºæ ‡è®°ã€‚

    :param df: è¾“å…¥çš„ Dask DataFrame
    :param index_column: ç´¢å¼•å­—æ®µå
    :param sort: æ˜¯å¦å¯ç”¨æ’åº
    :return: è®¾ç½®ç´¢å¼•åçš„ DataFrame æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    try:
        df_indexed = df.set_index(index_column, sorted=sort)
        logger.info(f"âœ… å·²è®¾ç½®ç´¢å¼•å­—æ®µï¼š{index_column}ï¼Œæ’åºï¼š{sort}")
        return df_indexed
    except Exception as e:
        logger.error(f"âŒ è®¾ç½®ç´¢å¼•å¤±è´¥ï¼š{e}")
        return None


def save_sorted_data(df: dd.DataFrame, output_file: str = 'sorted_data.parquet') -> bool:
    """
    å°†æ’åºåçš„æ•°æ®ä¿å­˜ä¸º Parquet æ–‡ä»¶ã€‚

    :param df: æ’åºåçš„ Dask DataFrame
    :param output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :return: æˆåŠŸä¸å¦
    """
    try:
        df.to_parquet(output_file)
        logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜è‡³ {output_file}")
        return True
    except Exception as e:
        logger.error(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥ï¼š{e}")
        return False


def run_pipeline(
    input_file: str = 'large_data.parquet',
    output_file: str = 'sorted_data.parquet',
    index_column: str = 'date',
    partition_size: str = '100MB',
    memory_limit: str = '32GB'
):
    """
    æ‰§è¡Œå®Œæ•´çš„â€œæŒ‰æ—¥æœŸç´¢å¼•â€æµæ°´çº¿ä»»åŠ¡ã€‚
    """
    start_time = time.time()

    # å¯åŠ¨ Dask å®¢æˆ·ç«¯
    # client = start_dask_client(memory_limit=memory_limit)
    # if client is None:
    #     return

    # é˜¶æ®µ1ï¼šåŠ è½½æ•°æ®
    load_start = time.time()
    ddf = load_parquet_data(input_file)
    if ddf is None:
        return
    load_end = time.time()
    logger.info(f"â±ï¸ æ•°æ®åŠ è½½è€—æ—¶ï¼š{load_end - load_start:.2f} ç§’")

    # é˜¶æ®µ2ï¼šé‡æ–°åˆ†åŒº
    repart_start = time.time()
    ddf_repart = repartition_data(df=ddf, partition_size=partition_size)
    if ddf_repart is None:
        return
    repart_end = time.time()
    logger.info(f"â±ï¸ æ•°æ®é‡æ–°åˆ†åŒºè€—æ—¶ï¼š{repart_end - repart_start:.2f} ç§’")

    # é˜¶æ®µ3ï¼šè®¾ç½®ç´¢å¼•
    index_start = time.time()
    ddf_sorted = set_date_index(df=ddf_repart, index_column=index_column, sort=True)
    if ddf_sorted is None:
        return
    index_end = time.time()
    logger.info(f"â±ï¸ è®¾ç½®ç´¢å¼•è€—æ—¶ï¼š{index_end - index_start:.2f} ç§’")

    # é˜¶æ®µ4ï¼šä¿å­˜ç»“æœ
    save_start = time.time()
    success = save_sorted_data(df=ddf_sorted, output_file=output_file)
    save_end = time.time()
    logger.info(f"â±ï¸ æ•°æ®ä¿å­˜è€—æ—¶ï¼š{save_end - save_start:.2f} ç§’")

    total_time = time.time() - start_time
    logger.info(f"âœ… æ€»è€—æ—¶ï¼š{total_time:.2f} ç§’")
    logger.info(f"ğŸš€ æµæ°´çº¿æ‰§è¡Œ {'æˆåŠŸ' if success else 'å¤±è´¥'}")

    # æ¸…ç†èµ„æº
    # client.close()


if __name__ == "__main__":
    run_pipeline(
        input_file='large_data.parquet',
        output_file='sorted_data.parquet',
        index_column='date',
        partition_size='100MB',
        memory_limit='32GB'
    )
